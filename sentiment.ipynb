{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbvHKaxEybqP"
   },
   "source": [
    "### Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4809,
     "status": "ok",
     "timestamp": 1670472275928,
     "user": {
      "displayName": "Josiah Coad",
      "userId": "12096942713822516722"
     },
     "user_tz": 360
    },
    "id": "TytpVVLzcVO8",
    "outputId": "1da954eb-97fe-434a-f466-7b34c4e54356"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "\n",
    "root_dir = \"/content/gdrive/My Drive/NLP Sentiment Analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 182,
     "status": "ok",
     "timestamp": 1670472276106,
     "user": {
      "displayName": "Josiah Coad",
      "userId": "12096942713822516722"
     },
     "user_tz": 360
    },
    "id": "Un9RO-iWbD-8"
   },
   "outputs": [],
   "source": [
    "with open(f'train_neg_reviews.txt',encoding='utf-8') as f:\n",
    "  contents = f.read()\n",
    "  train_neg_reviews = [review[len('4\\t'):] for review in contents.split('\\n')]\n",
    "\n",
    "with open(f'train_pos_reviews.txt',encoding='utf-8') as f:\n",
    "  contents = f.read()\n",
    "  train_pos_reviews = [review[len('4\\t'):] for review in contents.split('\\n')]\n",
    "\n",
    "with open(f'test_neg_reviews.txt',encoding='utf-8') as f:\n",
    "  contents = f.read()\n",
    "  test_neg_reviews = [review[len('4\\t'):] for review in contents.split('\\n')]\n",
    "\n",
    "with open(f'test_pos_reviews.txt',encoding='utf-8') as f:\n",
    "  contents = f.read()\n",
    "  test_pos_reviews = [review[len('4\\t'):] for review in contents.split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1670472276107,
     "user": {
      "displayName": "Josiah Coad",
      "userId": "12096942713822516722"
     },
     "user_tz": 360
    },
    "id": "PuxLXTMgfDvC"
   },
   "outputs": [],
   "source": [
    "train_docs = train_neg_reviews + train_pos_reviews\n",
    "y_train = [0]*len(train_neg_reviews) + [1]*len(train_pos_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 10952,
     "status": "ok",
     "timestamp": 1670472287057,
     "user": {
      "displayName": "Josiah Coad",
      "userId": "12096942713822516722"
     },
     "user_tz": 360
    },
    "id": "K9kCWCbdfniM"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(train_docs)\n",
    "X_train = vectorizer.transform(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 10918,
     "status": "ok",
     "timestamp": 1670472297972,
     "user": {
      "displayName": "Josiah Coad",
      "userId": "12096942713822516722"
     },
     "user_tz": 360
    },
    "id": "LqsT72ZvgMco"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12500, 56831) 12500 (25000, 56831) 25000 (37500, 56831) 37500\n"
     ]
    }
   ],
   "source": [
    "test_docs = test_neg_reviews + test_pos_reviews\n",
    "y_test = [0]*len(test_neg_reviews) + [1]*len(test_pos_reviews)\n",
    "X_test = vectorizer.transform(test_docs)\n",
    "\n",
    "\n",
    "\n",
    "X = vectorizer.transform(train_docs+test_docs)\n",
    "y = y_train + y_test\n",
    "\n",
    "print(X_train.shape, len(y_train), X_test.shape, len(y_test), X.shape, len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTKPXVGWf-Es"
   },
   "source": [
    "A random forest is a type of ensemble machine learning model that is made up of multiple decision trees. Ensemble models combine the predictions of multiple individual models to make more accurate predictions. In a random forest, each decision tree is trained on a random subset of the data, and the final prediction is made by averaging the predictions of all the individual decision trees.\n",
    "\n",
    "Here is an example of how to train a random forest using the scikit-learn library in Python:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhFWIkRCyhRN"
   },
   "source": [
    "## Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32528,
     "status": "ok",
     "timestamp": 1670472330497,
     "user": {
      "displayName": "Josiah Coad",
      "userId": "12096942713822516722"
     },
     "user_tz": 360
    },
    "id": "V-duzIGdfz_O",
    "outputId": "4725177a-a28b-43b1-c818-1949dcdf0e25"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a random forest classifier with 100 trees\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Train the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Score\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 826,
     "status": "ok",
     "timestamp": 1670472346376,
     "user": {
      "displayName": "Josiah Coad",
      "userId": "12096942713822516722"
     },
     "user_tz": 360
    },
    "id": "zhSlIap54YSN",
    "outputId": "7947c20f-2197-44af-f9f8-017498944587"
   },
   "outputs": [],
   "source": [
    "# get feature (word) importances\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dcsq87hTzGVd"
   },
   "source": [
    "Grid search can help us find the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1670468658622,
     "user": {
      "displayName": "Josiah Coad",
      "userId": "12096942713822516722"
     },
     "user_tz": 360
    },
    "id": "Hx8AcFm1kyaC"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the hyperparameter grid for the model\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 100, 1000],\n",
    "    'max_depth': [5, 10, 50, 100],\n",
    "    'min_impurity_decrease': [0, 0.1, 1],\n",
    "    'max_features': [1, 10, 100, 1000, None]\n",
    "}\n",
    "\n",
    "model_grid = RandomForestClassifier()\n",
    "\n",
    "# Use GridSearchCV to search for the best hyperparameters\n",
    "clf = GridSearchCV(model_grid, param_grid, cv=5)\n",
    "\n",
    "\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(f\"Best hyperparameters: {clf.best_params_}. Score: {clf.best_score_:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 67213,
     "status": "ok",
     "timestamp": 1670465813878,
     "user": {
      "displayName": "Josiah Coad",
      "userId": "12096942713822516722"
     },
     "user_tz": 360
    },
    "id": "Xb0XoZpbgp5f",
    "outputId": "2edbf1f0-b7f0-4a77-c2b3-fc4c41bdc71c"
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Create a gradient boosting classifier\n",
    "clf = GradientBoostingClassifier()\n",
    "\n",
    "# Train the classifier on the data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on new data\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AUb8MfvjyxIm"
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Create the XGBoost model\n",
    "model = xgb.XGBClassifier()\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9IMavwz9gt-"
   },
   "source": [
    "## Contextual Polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1054,
     "status": "ok",
     "timestamp": 1670473176163,
     "user": {
      "displayName": "Josiah Coad",
      "userId": "12096942713822516722"
     },
     "user_tz": 360
    },
    "id": "VmLoGsk28vMk",
    "outputId": "4d549b54-87e5-4f62-f6bb-d773b2e6bd2a"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "feature_names = np.array(vectorizer.get_feature_names())\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "print(\"Negative Words\", feature_names[sorted_coef_index[:10]])\n",
    "print(\"Positive Words\", feature_names[sorted_coef_index[-10:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "06O9ABt9opJn",
    "outputId": "60a45b63-3dd9-4181-82ea-179320a0633b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "names = [\n",
    "    #\"Nearest Neighbors\",\n",
    "    #\"Linear SVM\",\n",
    "    #\"RBF SVM\",\n",
    "    \"Gaussian Process\",\n",
    "    \"Decision Tree\",\n",
    "    \"Random Forest\",\n",
    "    \"Neural Net\",\n",
    "    \"AdaBoost\",\n",
    "    \"Naive Bayes\",\n",
    "    \"QDA\",\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    #KNeighborsClassifier(30),\n",
    "    #SVC(kernel=\"linear\", C=0.025),\n",
    "    #SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=50),\n",
    "    RandomForestClassifier(max_depth=50, n_estimators=100, max_features=10),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "]\n",
    "\n",
    "\n",
    "#figure = plt.figure(figsize=(27, 9))\n",
    "i = 1\n",
    "# preprocess dataset, split into training and test part\n",
    "\n",
    "X_normal = StandardScaler(with_mean=False).fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42\n",
    ")\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "\n",
    "scores = {}\n",
    "# iterate over classifiers\n",
    "for name, clf in zip(names, classifiers):\n",
    "    clf.fit(X_train.toarray(), y_train)\n",
    "    #score = clf.score(X_test, y_test)\n",
    "    print(name)#, score)\n",
    "    #scores[name] = score\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YMyHfdHz4QR_"
   },
   "outputs": [],
   "source": [
    "# doc2vec embeddings (instead of TF-IDF)\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# Create a list of TaggedDocument objects\n",
    "documents = [TaggedDocument(doc.split(), [i]) for i, doc in enumerate(train_docs)]\n",
    "\n",
    "# Initialize the Doc2Vec model\n",
    "model = Doc2Vec(vector_size=300, min_count=1, epochs=50)\n",
    "\n",
    "# Build the vocabulary\n",
    "model.build_vocab(documents)\n",
    "\n",
    "# Train the model\n",
    "model.train(documents, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# Generate vector representation for a document\n",
    "X_train = [model.infer_vector(doc) for doc in train_docs]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOBaYMe6daeLGTWO4s1kPk1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
